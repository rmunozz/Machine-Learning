Modifications made to fully_connected_feed.py:
Changed learning rate to 0.1
Changed steps to 3000
Changed hidden1 layer to 300
Changed hidden2 layer to 42
Changed batch size to 400

Output: 

**Note I was not able to reach <98% but I got very close. 

(tensorflow) eduroam-169-233-193-96:Supplemental_Material ricardomunoz$ python fully_connected_feed.py 
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Step 0: loss = 2.30 (0.060 sec)
Step 100: loss = 0.51 (0.023 sec)
Step 200: loss = 0.40 (0.024 sec)
Step 300: loss = 0.33 (0.029 sec)
Step 400: loss = 0.35 (0.023 sec)
Step 500: loss = 0.25 (0.025 sec)
Step 600: loss = 0.28 (0.026 sec)
Step 700: loss = 0.20 (0.023 sec)
Step 800: loss = 0.20 (0.023 sec)
Step 900: loss = 0.19 (0.022 sec)
Training Data Eval:
  Num examples: 54800  Num correct: 52060  Precision @ 1: 0.9500
Validation Data Eval:
  Num examples: 4800  Num correct: 4575  Precision @ 1: 0.9531
Test Data Eval:
  Num examples: 10000  Num correct: 9486  Precision @ 1: 0.9486
Step 1000: loss = 0.11 (0.036 sec)
Step 1100: loss = 0.16 (0.023 sec)
Step 1200: loss = 0.12 (0.022 sec)
Step 1300: loss = 0.15 (0.026 sec)
Step 1400: loss = 0.18 (0.023 sec)
Step 1500: loss = 0.14 (0.023 sec)
Step 1600: loss = 0.15 (0.024 sec)
Step 1700: loss = 0.08 (0.023 sec)
Step 1800: loss = 0.12 (0.022 sec)
Step 1900: loss = 0.10 (0.023 sec)
Training Data Eval:
  Num examples: 54800  Num correct: 53243  Precision @ 1: 0.9716
Validation Data Eval:
  Num examples: 4800  Num correct: 4650  Precision @ 1: 0.9688
Test Data Eval:
  Num examples: 10000  Num correct: 9664  Precision @ 1: 0.9664
Step 2000: loss = 0.08 (0.038 sec)
Step 2100: loss = 0.12 (0.025 sec)
Step 2200: loss = 0.09 (0.022 sec)
Step 2300: loss = 0.10 (0.028 sec)
Step 2400: loss = 0.07 (0.022 sec)
Step 2500: loss = 0.06 (0.023 sec)
Step 2600: loss = 0.07 (0.024 sec)
Step 2700: loss = 0.05 (0.024 sec)
Step 2800: loss = 0.06 (0.023 sec)
Step 2900: loss = 0.10 (0.028 sec)
Training Data Eval:
  Num examples: 54800  Num correct: 53836  Precision @ 1: 0.9824
Validation Data Eval:
  Num examples: 4800  Num correct: 4673  Precision @ 1: 0.9735
Test Data Eval:
  Num examples: 10000  Num correct: 9726  Precision @ 1: 0.9726